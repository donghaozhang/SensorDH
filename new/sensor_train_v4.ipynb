{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch as tf\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tools import load_sensor_data_without_h, sample_sensor_data, manual_lable_array_list, most_frequent, extract_sensor_data, get_sensor_data, sample_data, sample_label, combine_to_one\n",
    "from model import Classifier_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, feature_len = 39, seq_len = 128, num_classes = 6, dim = 1024, depth = 6, heads = 8, mlp_dim = 2048, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.5, emb_dropout = 0.5):\n",
    "        super().__init__()\n",
    "        #assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = seq_len\n",
    "        patch_dim = feature_len\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        #self.to_patch_embedding = nn.Sequential(\n",
    "           #Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            #nn.Linear(patch_dim, dim),\n",
    "        #)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_to_label = {'talk':0, 'eat':1, 'read':2, 'drink':3, 'computer':4, 'write':5, 'other': 6}\n",
    "actor_1 = [1,2,3,4,5,6,7,8,9,10]\n",
    "actor_2 = [11,12,13,14,31,32,33,34,35,36]\n",
    "actor_3 = [15,16,17,18,37,38,39,40,41,42]\n",
    "actor_4 = [43,44,45,46,47,48,49,50,51,52]\n",
    "actor_5 = [53,54,55,56,57,58,59,60,61,62]\n",
    "actor_6 = [63,64,65,66,67,68,69,70,71,72]\n",
    "actor_7 = [76,77,78,79,80,81,82,83,84,85]\n",
    "actor_8 = [86,87,88,89,90,91,92,93,94,95]\n",
    "actor_9 = [97,98,99,100,101,102,103,104,105,106]\n",
    "actor_10 = [107,108,109,110,111,112,113,114,115,116]\n",
    "actor_11 = [117,118,119,120,121,122,123,124,125,126]\n",
    "actor_12 = [128,129,130,131,132,133,134,135,136,137]\n",
    "actor_13 = [138,139,140,141,142,143,144,145,146,147]\n",
    "actor_14 = [148,149,150,151,152,153,154,155,156,157]\n",
    "actor_15 = [161,162,163,164,165,166,167,168,169,170]\n",
    "actor_16 = [171,172,173,174,175,176,177,178,179,180]\n",
    "actor_17 = [181,182,183,184,185,186,187,188,189,190]\n",
    "actor_18 = [191,192,193,194,195,196,197,198,199,200]\n",
    "actor_19 = [201,202,203,204,205,206,207,208,209,210]\n",
    "actor_20 = [212,213,214,215,216,217,218,219,220,221]\n",
    "actor_21 = [223,224,225,226,227,228,229,230,231,232]\n",
    "actor_22 = [233,234,235,236,237,238,239,240,241,242]\n",
    "actor_23 = [243,244,245,246,247,248,249,250,251,252]\n",
    "actor_24 = [254,255,256,257,258,259,260,261,262,263]\n",
    "actor_25 = [264,265,266,267,268,269,270,271,272,273]\n",
    "combine_list = actor_1 + actor_2 + actor_3 + actor_4 + actor_5 + actor_6 + actor_7 + actor_8 + actor_9 + actor_10+ actor_11 + actor_12 + actor_13 + actor_14 + actor_15+ actor_16 + actor_17 + actor_18 + actor_19 + actor_20+ actor_21 + actor_22 + actor_23 + actor_24 + actor_25\n",
    "window_sz = 128\n",
    "sample_sz = 128\n",
    "lr = 0.0005\n",
    "n_epochs = 20\n",
    "num_classes = 6\n",
    "patience, trials = 5, 0\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "bs = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual_lable_array_list('../manual_lable.csv', combine_list)\n",
    "#extract_sensor_data(combine_list) \n",
    "\n",
    "#label_list = np.load('../save_data/label_data/label_list.npy', allow_pickle=True).item()\n",
    "#sensor_data = get_sensor_data('../save_data/original_data/')\n",
    "\n",
    "#sample_data(sensor_data, combine_list, window_sz = window_sz, sample_sz = sample_sz)\n",
    "#sample_label(label_list, combine_list, window_sz = window_sz, sample_sz = sample_sz)\n",
    "   \n",
    "sample_sensor_data = get_sensor_data('../save_data/sample_data/')\n",
    "sample_sensor_label = get_sensor_data('../save_data/sample_label/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 128, 39)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sensor_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_list = total_num_list = actor_1 + actor_2 + actor_3 + actor_4+actor_5+actor_6+actor_7+actor_8+actor_9+actor_10+actor_11+actor_12+actor_13+actor_14+actor_15+actor_16+actor_17+actor_18+actor_19+actor_20+actor_21+actor_22+actor_23+actor_24+actor_25\n",
    "val_num_list = total_num_list[0*10 : 0*10+10]\n",
    "del total_num_list[0*10 : 0*10+10]\n",
    "train_num_list = total_num_list\n",
    "train_sensor_data = combine_to_one(sample_sensor_data,train_num_list)\n",
    "val_sensor_data = combine_to_one(sample_sensor_data,val_num_list)\n",
    "train_sensor_label = combine_to_one(sample_sensor_label,train_num_list)\n",
    "val_sensor_label = combine_to_one(sample_sensor_label,val_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(torch.tensor(train_sensor_data).float(), torch.tensor(train_sensor_label).long())\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=0)\n",
    "val_ds = TensorDataset(torch.tensor(val_sensor_data).float(), torch.tensor(val_sensor_label).long())\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch:   1. Loss: 102768.8804. Acc.: 40.18%\n",
      "Epoch 1 best model saved with accuracy: 40.18%\n",
      "Epoch:   2. Loss: 99150.2301. Acc.: 35.05%\n",
      "Epoch:   3. Loss: 93951.4135. Acc.: 41.49%\n",
      "Epoch 3 best model saved with accuracy: 41.49%\n",
      "Epoch:   4. Loss: 83795.4027. Acc.: 32.53%\n",
      "Epoch:   5. Loss: 84446.7891. Acc.: 39.22%\n",
      "Epoch:   6. Loss: 78012.5945. Acc.: 26.62%\n",
      "Epoch:   7. Loss: 76216.8160. Acc.: 21.92%\n",
      "Epoch:   8. Loss: 77803.0577. Acc.: 23.02%\n",
      "Epoch:   9. Loss: 73704.7116. Acc.: 22.56%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d8cc381a50d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ViT().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "best_acc = 0\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "print('Start model training')\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        #print(i)\n",
    "        x_raw, y_batch = [t.to(device) for t in batch]\n",
    "        y_batch = tf.squeeze(y_batch)\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw)\n",
    "        loss = criterion(out, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    loss_history.append(epoch_loss)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in val_dl:\n",
    "        x_raw, y_batch = [t.to(device) for t in batch]\n",
    "        y_batch = tf.squeeze(y_batch)\n",
    "        out = model(x_raw)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        if preds.size()[0] > 1:\n",
    "            total += y_batch.size(0)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "    acc = correct / total\n",
    "    acc_history.append(acc)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch:   1. Loss: 89859.1187. Acc.: 35.80%\n",
      "Epoch 1 best model saved with accuracy: 35.80%\n",
      "Epoch:   2. Loss: 66338.9857. Acc.: 39.61%\n",
      "Epoch 2 best model saved with accuracy: 39.61%\n",
      "Epoch:   3. Loss: 62750.2263. Acc.: 42.49%\n",
      "Epoch 3 best model saved with accuracy: 42.49%\n",
      "Epoch:   4. Loss: 60114.0228. Acc.: 32.46%\n",
      "Epoch:   5. Loss: 59316.0874. Acc.: 32.78%\n",
      "Epoch:   6. Loss: 57335.0363. Acc.: 33.63%\n",
      "Epoch:   7. Loss: 56096.8383. Acc.: 35.77%\n",
      "Epoch:   8. Loss: 55937.0582. Acc.: 32.67%\n",
      "Early stopping on epoch 8\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88599.3430. Acc.: 39.43%\n",
      "Epoch 1 best model saved with accuracy: 39.43%\n",
      "Epoch:   2. Loss: 64715.4332. Acc.: 38.65%\n",
      "Epoch:   3. Loss: 61359.9303. Acc.: 41.49%\n",
      "Epoch 3 best model saved with accuracy: 41.49%\n",
      "Epoch:   4. Loss: 59558.7249. Acc.: 35.30%\n",
      "Epoch:   5. Loss: 58747.6288. Acc.: 38.15%\n",
      "Epoch:   6. Loss: 58104.2313. Acc.: 38.90%\n",
      "Epoch:   7. Loss: 57465.2102. Acc.: 34.80%\n",
      "Epoch:   8. Loss: 58354.1912. Acc.: 37.26%\n",
      "Early stopping on epoch 8\n",
      "Start model training\n",
      "Epoch:   1. Loss: 89126.0698. Acc.: 63.56%\n",
      "Epoch 1 best model saved with accuracy: 63.56%\n",
      "Epoch:   2. Loss: 67351.7861. Acc.: 55.48%\n",
      "Epoch:   3. Loss: 64443.1437. Acc.: 50.96%\n",
      "Epoch:   4. Loss: 63060.5623. Acc.: 51.60%\n",
      "Epoch:   5. Loss: 62277.7319. Acc.: 54.23%\n",
      "Epoch:   6. Loss: 60257.9424. Acc.: 57.86%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88611.7635. Acc.: 38.58%\n",
      "Epoch 1 best model saved with accuracy: 38.58%\n",
      "Epoch:   2. Loss: 65738.4860. Acc.: 35.34%\n",
      "Epoch:   3. Loss: 61237.4057. Acc.: 29.15%\n",
      "Epoch:   4. Loss: 59241.0871. Acc.: 31.60%\n",
      "Epoch:   5. Loss: 58723.1633. Acc.: 35.91%\n",
      "Epoch:   6. Loss: 56283.9367. Acc.: 30.71%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 89448.6263. Acc.: 66.55%\n",
      "Epoch 1 best model saved with accuracy: 66.55%\n",
      "Epoch:   2. Loss: 67702.3518. Acc.: 64.34%\n",
      "Epoch:   3. Loss: 63219.5227. Acc.: 66.48%\n",
      "Epoch:   4. Loss: 62924.8924. Acc.: 62.35%\n",
      "Epoch:   5. Loss: 61644.6060. Acc.: 62.06%\n",
      "Epoch:   6. Loss: 60342.3795. Acc.: 61.46%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88901.9082. Acc.: 50.85%\n",
      "Epoch 1 best model saved with accuracy: 50.85%\n",
      "Epoch:   2. Loss: 68287.4297. Acc.: 54.16%\n",
      "Epoch 2 best model saved with accuracy: 54.16%\n",
      "Epoch:   3. Loss: 64089.5997. Acc.: 55.52%\n",
      "Epoch 3 best model saved with accuracy: 55.52%\n",
      "Epoch:   4. Loss: 62573.4263. Acc.: 52.67%\n",
      "Epoch:   5. Loss: 61418.6332. Acc.: 56.51%\n",
      "Epoch 5 best model saved with accuracy: 56.51%\n",
      "Epoch:   6. Loss: 60489.5098. Acc.: 55.55%\n",
      "Epoch:   7. Loss: 59598.6177. Acc.: 54.20%\n",
      "Epoch:   8. Loss: 58072.8508. Acc.: 58.01%\n",
      "Epoch 8 best model saved with accuracy: 58.01%\n",
      "Epoch:   9. Loss: 58119.8488. Acc.: 51.49%\n",
      "Epoch:  10. Loss: 56861.4512. Acc.: 56.01%\n",
      "Epoch:  11. Loss: 56096.0296. Acc.: 57.94%\n",
      "Epoch:  12. Loss: 56245.4916. Acc.: 52.31%\n",
      "Epoch:  13. Loss: 55676.6494. Acc.: 54.52%\n",
      "Early stopping on epoch 13\n",
      "Start model training\n",
      "Epoch:   1. Loss: 90613.0283. Acc.: 65.62%\n",
      "Epoch 1 best model saved with accuracy: 65.62%\n",
      "Epoch:   2. Loss: 67257.3789. Acc.: 73.56%\n",
      "Epoch 2 best model saved with accuracy: 73.56%\n",
      "Epoch:   3. Loss: 63229.4267. Acc.: 72.88%\n",
      "Epoch:   4. Loss: 61855.7209. Acc.: 70.46%\n",
      "Epoch:   5. Loss: 61285.3368. Acc.: 69.15%\n",
      "Epoch:   6. Loss: 60109.1713. Acc.: 70.04%\n",
      "Epoch:   7. Loss: 58833.6168. Acc.: 71.46%\n",
      "Early stopping on epoch 7\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88155.9378. Acc.: 48.93%\n",
      "Epoch 1 best model saved with accuracy: 48.93%\n",
      "Epoch:   2. Loss: 65370.9192. Acc.: 45.62%\n",
      "Epoch:   3. Loss: 62800.7019. Acc.: 48.93%\n",
      "Epoch:   4. Loss: 61366.7068. Acc.: 46.80%\n",
      "Epoch:   5. Loss: 61298.3000. Acc.: 46.16%\n",
      "Epoch:   6. Loss: 60006.9980. Acc.: 47.19%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87880.5811. Acc.: 44.06%\n",
      "Epoch 1 best model saved with accuracy: 44.06%\n",
      "Epoch:   2. Loss: 65506.3030. Acc.: 43.45%\n",
      "Epoch:   3. Loss: 61564.1007. Acc.: 50.96%\n",
      "Epoch 3 best model saved with accuracy: 50.96%\n",
      "Epoch:   4. Loss: 60564.5581. Acc.: 49.47%\n",
      "Epoch:   5. Loss: 57724.2123. Acc.: 46.33%\n",
      "Epoch:   6. Loss: 56685.0317. Acc.: 43.06%\n",
      "Epoch:   7. Loss: 56270.9323. Acc.: 47.76%\n",
      "Epoch:   8. Loss: 55794.6848. Acc.: 45.37%\n",
      "Early stopping on epoch 8\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87966.9884. Acc.: 61.71%\n",
      "Epoch 1 best model saved with accuracy: 61.71%\n",
      "Epoch:   2. Loss: 65917.2306. Acc.: 63.59%\n",
      "Epoch 2 best model saved with accuracy: 63.59%\n",
      "Epoch:   3. Loss: 61998.0282. Acc.: 63.17%\n",
      "Epoch:   4. Loss: 61372.5135. Acc.: 64.31%\n",
      "Epoch 4 best model saved with accuracy: 64.31%\n",
      "Epoch:   5. Loss: 60511.8844. Acc.: 57.30%\n",
      "Epoch:   6. Loss: 59948.5585. Acc.: 49.64%\n",
      "Epoch:   7. Loss: 58741.7777. Acc.: 57.69%\n",
      "Epoch:   8. Loss: 57830.0326. Acc.: 60.71%\n",
      "Epoch:   9. Loss: 57021.7116. Acc.: 60.25%\n",
      "Early stopping on epoch 9\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88610.4554. Acc.: 50.50%\n",
      "Epoch 1 best model saved with accuracy: 50.50%\n",
      "Epoch:   2. Loss: 68404.9187. Acc.: 50.60%\n",
      "Epoch 2 best model saved with accuracy: 50.60%\n",
      "Epoch:   3. Loss: 65415.0166. Acc.: 55.34%\n",
      "Epoch 3 best model saved with accuracy: 55.34%\n",
      "Epoch:   4. Loss: 63289.4585. Acc.: 42.35%\n",
      "Epoch:   5. Loss: 62520.3438. Acc.: 56.62%\n",
      "Epoch 5 best model saved with accuracy: 56.62%\n",
      "Epoch:   6. Loss: 60341.5905. Acc.: 58.75%\n",
      "Epoch 6 best model saved with accuracy: 58.75%\n",
      "Epoch:   7. Loss: 60382.6419. Acc.: 52.63%\n",
      "Epoch:   8. Loss: 60423.9600. Acc.: 51.49%\n",
      "Epoch:   9. Loss: 59384.4109. Acc.: 53.63%\n",
      "Epoch:  10. Loss: 58168.8328. Acc.: 54.80%\n",
      "Epoch:  11. Loss: 58129.7941. Acc.: 56.33%\n",
      "Early stopping on epoch 11\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87555.1571. Acc.: 52.46%\n",
      "Epoch 1 best model saved with accuracy: 52.46%\n",
      "Epoch:   2. Loss: 67625.9871. Acc.: 54.02%\n",
      "Epoch 2 best model saved with accuracy: 54.02%\n",
      "Epoch:   3. Loss: 62841.6392. Acc.: 45.05%\n",
      "Epoch:   4. Loss: 61972.4237. Acc.: 43.56%\n",
      "Epoch:   5. Loss: 60879.0407. Acc.: 48.29%\n",
      "Epoch:   6. Loss: 59005.4236. Acc.: 42.78%\n",
      "Epoch:   7. Loss: 58782.6416. Acc.: 48.29%\n",
      "Early stopping on epoch 7\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87725.0484. Acc.: 46.65%\n",
      "Epoch 1 best model saved with accuracy: 46.65%\n",
      "Epoch:   2. Loss: 65986.8041. Acc.: 49.04%\n",
      "Epoch 2 best model saved with accuracy: 49.04%\n",
      "Epoch:   3. Loss: 61871.2685. Acc.: 50.82%\n",
      "Epoch 3 best model saved with accuracy: 50.82%\n",
      "Epoch:   4. Loss: 61356.2806. Acc.: 51.39%\n",
      "Epoch 4 best model saved with accuracy: 51.39%\n",
      "Epoch:   5. Loss: 60564.3476. Acc.: 46.30%\n",
      "Epoch:   6. Loss: 58844.7788. Acc.: 51.17%\n",
      "Epoch:   7. Loss: 57045.8124. Acc.: 45.05%\n",
      "Epoch:   8. Loss: 56979.0006. Acc.: 49.47%\n",
      "Epoch:   9. Loss: 57788.4151. Acc.: 50.32%\n",
      "Early stopping on epoch 9\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88670.5254. Acc.: 54.31%\n",
      "Epoch 1 best model saved with accuracy: 54.31%\n",
      "Epoch:   2. Loss: 65922.2264. Acc.: 48.29%\n",
      "Epoch:   3. Loss: 62484.8873. Acc.: 53.67%\n",
      "Epoch:   4. Loss: 60086.6827. Acc.: 53.56%\n",
      "Epoch:   5. Loss: 59845.5072. Acc.: 52.78%\n",
      "Epoch:   6. Loss: 58879.4851. Acc.: 52.35%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88424.4043. Acc.: 56.90%\n",
      "Epoch 1 best model saved with accuracy: 56.90%\n",
      "Epoch:   2. Loss: 66597.6962. Acc.: 56.87%\n",
      "Epoch:   3. Loss: 64139.4977. Acc.: 55.12%\n",
      "Epoch:   4. Loss: 62123.3202. Acc.: 53.74%\n",
      "Epoch:   5. Loss: 61031.1739. Acc.: 57.12%\n",
      "Epoch 5 best model saved with accuracy: 57.12%\n",
      "Epoch:   6. Loss: 59327.3168. Acc.: 49.89%\n",
      "Epoch:   7. Loss: 58554.0309. Acc.: 54.98%\n",
      "Epoch:   8. Loss: 58915.6245. Acc.: 51.57%\n",
      "Epoch:   9. Loss: 57810.1529. Acc.: 55.84%\n",
      "Epoch:  10. Loss: 57435.1515. Acc.: 53.42%\n",
      "Early stopping on epoch 10\n",
      "Start model training\n",
      "Epoch:   1. Loss: 89787.1677. Acc.: 42.67%\n",
      "Epoch 1 best model saved with accuracy: 42.67%\n",
      "Epoch:   2. Loss: 67332.0692. Acc.: 38.86%\n",
      "Epoch:   3. Loss: 63531.4729. Acc.: 45.48%\n",
      "Epoch 3 best model saved with accuracy: 45.48%\n",
      "Epoch:   4. Loss: 63239.0136. Acc.: 41.21%\n",
      "Epoch:   5. Loss: 62209.7154. Acc.: 49.25%\n",
      "Epoch 5 best model saved with accuracy: 49.25%\n",
      "Epoch:   6. Loss: 60114.2459. Acc.: 43.91%\n",
      "Epoch:   7. Loss: 59142.5250. Acc.: 48.75%\n",
      "Epoch:   8. Loss: 58201.9725. Acc.: 45.80%\n",
      "Epoch:   9. Loss: 58478.0805. Acc.: 47.26%\n",
      "Epoch:  10. Loss: 56808.9197. Acc.: 49.47%\n",
      "Epoch 10 best model saved with accuracy: 49.47%\n",
      "Epoch:  11. Loss: 55921.4122. Acc.: 44.02%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12. Loss: 56484.6980. Acc.: 43.42%\n",
      "Epoch:  13. Loss: 56057.9506. Acc.: 39.47%\n",
      "Epoch:  14. Loss: 55568.2661. Acc.: 40.50%\n",
      "Epoch:  15. Loss: 55469.4382. Acc.: 47.19%\n",
      "Early stopping on epoch 15\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87124.0948. Acc.: 49.47%\n",
      "Epoch 1 best model saved with accuracy: 49.47%\n",
      "Epoch:   2. Loss: 66949.9420. Acc.: 47.69%\n",
      "Epoch:   3. Loss: 62820.7975. Acc.: 51.25%\n",
      "Epoch 3 best model saved with accuracy: 51.25%\n",
      "Epoch:   4. Loss: 60893.6987. Acc.: 47.47%\n",
      "Epoch:   5. Loss: 59212.5166. Acc.: 48.40%\n",
      "Epoch:   6. Loss: 58270.9765. Acc.: 54.13%\n",
      "Epoch 6 best model saved with accuracy: 54.13%\n",
      "Epoch:   7. Loss: 58708.6648. Acc.: 48.47%\n",
      "Epoch:   8. Loss: 56264.2122. Acc.: 51.21%\n",
      "Epoch:   9. Loss: 55041.4898. Acc.: 48.11%\n",
      "Epoch:  10. Loss: 54995.2897. Acc.: 50.78%\n",
      "Epoch:  11. Loss: 53880.5732. Acc.: 46.05%\n",
      "Early stopping on epoch 11\n",
      "Start model training\n",
      "Epoch:   1. Loss: 86931.8144. Acc.: 51.17%\n",
      "Epoch 1 best model saved with accuracy: 51.17%\n",
      "Epoch:   2. Loss: 65909.7580. Acc.: 50.57%\n",
      "Epoch:   3. Loss: 62988.1550. Acc.: 57.72%\n",
      "Epoch 3 best model saved with accuracy: 57.72%\n",
      "Epoch:   4. Loss: 60780.9691. Acc.: 51.07%\n",
      "Epoch:   5. Loss: 59074.6895. Acc.: 51.25%\n",
      "Epoch:   6. Loss: 57504.7280. Acc.: 51.14%\n",
      "Epoch:   7. Loss: 57275.1368. Acc.: 44.34%\n",
      "Epoch:   8. Loss: 56446.6568. Acc.: 49.93%\n",
      "Early stopping on epoch 8\n",
      "Start model training\n",
      "Epoch:   1. Loss: 86616.8079. Acc.: 44.95%\n",
      "Epoch 1 best model saved with accuracy: 44.95%\n",
      "Epoch:   2. Loss: 64426.4598. Acc.: 46.55%\n",
      "Epoch 2 best model saved with accuracy: 46.55%\n",
      "Epoch:   3. Loss: 60677.0319. Acc.: 41.99%\n",
      "Epoch:   4. Loss: 59155.0919. Acc.: 37.30%\n",
      "Epoch:   5. Loss: 58467.5697. Acc.: 34.95%\n",
      "Epoch:   6. Loss: 58254.3693. Acc.: 39.96%\n",
      "Epoch:   7. Loss: 57182.8103. Acc.: 39.86%\n",
      "Early stopping on epoch 7\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87751.0776. Acc.: 38.40%\n",
      "Epoch 1 best model saved with accuracy: 38.40%\n",
      "Epoch:   2. Loss: 66463.7405. Acc.: 35.98%\n",
      "Epoch:   3. Loss: 63630.1928. Acc.: 40.28%\n",
      "Epoch 3 best model saved with accuracy: 40.28%\n",
      "Epoch:   4. Loss: 61378.5394. Acc.: 41.00%\n",
      "Epoch 4 best model saved with accuracy: 41.00%\n",
      "Epoch:   5. Loss: 59922.9593. Acc.: 30.39%\n",
      "Epoch:   6. Loss: 60714.9181. Acc.: 42.67%\n",
      "Epoch 6 best model saved with accuracy: 42.67%\n",
      "Epoch:   7. Loss: 61433.3581. Acc.: 31.03%\n",
      "Epoch:   8. Loss: 60551.9692. Acc.: 48.61%\n",
      "Epoch 8 best model saved with accuracy: 48.61%\n",
      "Epoch:   9. Loss: 59944.0006. Acc.: 40.53%\n",
      "Epoch:  10. Loss: 58760.4783. Acc.: 36.19%\n",
      "Epoch:  11. Loss: 58203.7564. Acc.: 37.44%\n",
      "Epoch:  12. Loss: 57556.8395. Acc.: 29.00%\n",
      "Epoch:  13. Loss: 58011.7111. Acc.: 37.62%\n",
      "Early stopping on epoch 13\n",
      "Start model training\n",
      "Epoch:   1. Loss: 86732.4758. Acc.: 50.39%\n",
      "Epoch 1 best model saved with accuracy: 50.39%\n",
      "Epoch:   2. Loss: 64260.2899. Acc.: 56.51%\n",
      "Epoch 2 best model saved with accuracy: 56.51%\n",
      "Epoch:   3. Loss: 60647.2300. Acc.: 52.78%\n",
      "Epoch:   4. Loss: 59255.8207. Acc.: 52.49%\n",
      "Epoch:   5. Loss: 57423.7883. Acc.: 55.37%\n",
      "Epoch:   6. Loss: 57538.7806. Acc.: 52.42%\n",
      "Epoch:   7. Loss: 57876.3572. Acc.: 57.72%\n",
      "Epoch 7 best model saved with accuracy: 57.72%\n",
      "Epoch:   8. Loss: 55557.5437. Acc.: 54.13%\n",
      "Epoch:   9. Loss: 56569.5372. Acc.: 52.28%\n",
      "Epoch:  10. Loss: 55991.7166. Acc.: 50.64%\n",
      "Epoch:  11. Loss: 54744.7566. Acc.: 52.85%\n",
      "Epoch:  12. Loss: 54029.6916. Acc.: 50.64%\n",
      "Early stopping on epoch 12\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87638.8481. Acc.: 46.19%\n",
      "Epoch 1 best model saved with accuracy: 46.19%\n",
      "Epoch:   2. Loss: 66162.4469. Acc.: 43.13%\n",
      "Epoch:   3. Loss: 62491.5058. Acc.: 36.87%\n",
      "Epoch:   4. Loss: 59585.6404. Acc.: 43.95%\n",
      "Epoch:   5. Loss: 57648.2766. Acc.: 44.06%\n",
      "Epoch:   6. Loss: 57967.0083. Acc.: 43.49%\n",
      "Early stopping on epoch 6\n",
      "Start model training\n",
      "Epoch:   1. Loss: 87215.8936. Acc.: 37.54%\n",
      "Epoch 1 best model saved with accuracy: 37.54%\n",
      "Epoch:   2. Loss: 64381.5479. Acc.: 38.72%\n",
      "Epoch 2 best model saved with accuracy: 38.72%\n",
      "Epoch:   3. Loss: 60960.5376. Acc.: 39.22%\n",
      "Epoch 3 best model saved with accuracy: 39.22%\n",
      "Epoch:   4. Loss: 58764.7181. Acc.: 41.32%\n",
      "Epoch 4 best model saved with accuracy: 41.32%\n",
      "Epoch:   5. Loss: 57726.8600. Acc.: 41.14%\n",
      "Epoch:   6. Loss: 56402.1475. Acc.: 41.46%\n",
      "Epoch 6 best model saved with accuracy: 41.46%\n",
      "Epoch:   7. Loss: 57145.3564. Acc.: 41.67%\n",
      "Epoch 7 best model saved with accuracy: 41.67%\n",
      "Epoch:   8. Loss: 55788.4579. Acc.: 43.10%\n",
      "Epoch 8 best model saved with accuracy: 43.10%\n",
      "Epoch:   9. Loss: 55250.0504. Acc.: 39.32%\n",
      "Epoch:  10. Loss: 54988.7967. Acc.: 39.54%\n",
      "Epoch:  11. Loss: 54944.4520. Acc.: 39.86%\n",
      "Epoch:  12. Loss: 54153.0450. Acc.: 34.48%\n",
      "Epoch:  13. Loss: 53557.7822. Acc.: 37.47%\n",
      "Early stopping on epoch 13\n",
      "Start model training\n",
      "Epoch:   1. Loss: 88638.1428. Acc.: 57.37%\n",
      "Epoch 1 best model saved with accuracy: 57.37%\n",
      "Epoch:   2. Loss: 66076.0163. Acc.: 62.28%\n",
      "Epoch 2 best model saved with accuracy: 62.28%\n",
      "Epoch:   3. Loss: 61673.3630. Acc.: 56.58%\n",
      "Epoch:   4. Loss: 58515.9167. Acc.: 58.19%\n",
      "Epoch:   5. Loss: 58380.2776. Acc.: 54.59%\n",
      "Epoch:   6. Loss: 57312.7515. Acc.: 60.14%\n",
      "Epoch:   7. Loss: 57433.8021. Acc.: 55.59%\n",
      "Early stopping on epoch 7\n",
      "Start model training\n",
      "Epoch:   1. Loss: 86219.7438. Acc.: 32.95%\n",
      "Epoch 1 best model saved with accuracy: 32.95%\n",
      "Epoch:   2. Loss: 63497.5929. Acc.: 37.19%\n",
      "Epoch 2 best model saved with accuracy: 37.19%\n",
      "Epoch:   3. Loss: 59872.2450. Acc.: 31.74%\n",
      "Epoch:   4. Loss: 59312.8460. Acc.: 41.21%\n",
      "Epoch 4 best model saved with accuracy: 41.21%\n",
      "Epoch:   5. Loss: 57348.2647. Acc.: 36.58%\n",
      "Epoch:   6. Loss: 56141.8218. Acc.: 36.65%\n",
      "Epoch:   7. Loss: 56872.0045. Acc.: 39.50%\n",
      "Epoch:   8. Loss: 55454.6245. Acc.: 29.40%\n",
      "Epoch:   9. Loss: 55258.5826. Acc.: 34.80%\n",
      "Early stopping on epoch 9\n"
     ]
    }
   ],
   "source": [
    "bestacc = []\n",
    "for actor in range(0,25):\n",
    "    total_num_list = total_num_list = actor_1 + actor_2 + actor_3 + actor_4+actor_5+actor_6+actor_7+actor_8+actor_9+actor_10+actor_11+actor_12+actor_13+actor_14+actor_15+actor_16+actor_17+actor_18+actor_19+actor_20+actor_21+actor_22+actor_23+actor_24+actor_25\n",
    "    val_num_list = total_num_list[actor*10 : actor*10+10]\n",
    "    del total_num_list[actor*10 : actor*10+10]\n",
    "    train_num_list = total_num_list\n",
    "    train_sensor_data = combine_to_one(sample_sensor_data,train_num_list)\n",
    "    val_sensor_data = combine_to_one(sample_sensor_data,val_num_list)\n",
    "    train_sensor_label = combine_to_one(sample_sensor_label,train_num_list)\n",
    "    val_sensor_label = combine_to_one(sample_sensor_label,val_num_list)\n",
    "    model = ViT().to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_ds = TensorDataset(torch.tensor(train_sensor_data).float(), torch.tensor(train_sensor_label).long())\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    val_ds = TensorDataset(torch.tensor(val_sensor_data).float(), torch.tensor(val_sensor_label).long())\n",
    "    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    best_acc = 0\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    print('Start model training')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            x_raw, y_batch = [t.to(device) for t in batch]\n",
    "            y_batch = tf.squeeze(y_batch)\n",
    "            opt.zero_grad()\n",
    "            out = model(x_raw)\n",
    "            loss = criterion(out, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        loss_history.append(epoch_loss)\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for batch in val_dl:\n",
    "            x_raw, y_batch = [t.to(device) for t in batch]\n",
    "            y_batch = tf.squeeze(y_batch)\n",
    "            out = model(x_raw)\n",
    "            preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "            if preds.size()[0] > 1:\n",
    "                total += y_batch.size(0)\n",
    "                correct += (preds == y_batch).sum().item()\n",
    "        acc = correct / total\n",
    "        acc_history.append(acc)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n",
    "        if acc > best_acc:\n",
    "            trials = 0\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best.pth')\n",
    "            print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "    bestacc.append(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42491103202846975,\n",
       " 0.41494661921708187,\n",
       " 0.6355871886120996,\n",
       " 0.38576512455516015,\n",
       " 0.6654804270462633,\n",
       " 0.5800711743772242,\n",
       " 0.7355871886120996,\n",
       " 0.4893238434163701,\n",
       " 0.5096085409252669,\n",
       " 0.6430604982206406,\n",
       " 0.5875444839857651,\n",
       " 0.5402135231316726,\n",
       " 0.5138790035587188,\n",
       " 0.5430604982206406,\n",
       " 0.5711743772241993,\n",
       " 0.49466192170818507,\n",
       " 0.5412811387900356,\n",
       " 0.5772241992882562,\n",
       " 0.46548042704626336,\n",
       " 0.48612099644128115,\n",
       " 0.5772241992882562,\n",
       " 0.4619217081850534,\n",
       " 0.4309608540925267,\n",
       " 0.6227758007117438,\n",
       " 0.4120996441281139]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[0.47259786476868326,\n",
    " 0.43843416370106764,\n",
    " 0.6220640569395017,\n",
    " 0.400355871886121,\n",
    " 0.6935943060498221,\n",
    " 0.5562277580071174,\n",
    " 0.7377224199288256,\n",
    " 0.5391459074733096,\n",
    " 0.4185053380782918,\n",
    " 0.6377224199288256,\n",
    " 0.6259786476868328,\n",
    " 0.5224199288256228,\n",
    " 0.5355871886120996,\n",
    " 0.5868327402135232,\n",
    " 0.5430604982206406,\n",
    " 0.5274021352313167,\n",
    " 0.5224199288256228,\n",
    " 0.6491103202846975,0\n",
    " 0.5096085409252669,\n",
    " 0.5494661921708185,\n",
    " 0.49110320284697506,\n",
    " 0.5099644128113879,\n",
    " 0.34483985765124553,\n",
    " 0.5587188612099644,\n",
    " 0.36725978647686836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534405693950178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5323985765124555"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bestacc)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[39.86, 54.80, 67.65, 44.23, 69.04, 65.87, 70.82, 53.27, 51.71, 69.40, 68.90, 59.82, 56.69, 69.07, 62.38, 60.32, 56.98, 73.74, 51.07, 64.84, 58.97, 51.25, 48.26, 52.63, 37.4.0.0007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[44.98, 41.00, 66.73, 45.05, 72.85, 66.48, 75.37, 54.20, 53.67, 69.50, 67.97, 52.28, 60.71, 63.99, 60.50, 57.37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(A)/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[54.80, 67.65, 44.23, 69.04, 65.87, 70.82, 53.27, 51.71, 69.40, 68.90, 59.82, 56.69, 69.07, 62.38, 60.32, 56.98, 73.74, 51.07, 64.84, 58.97, 51.25, 48.26, 52.63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(A)/25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
